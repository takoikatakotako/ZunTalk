import SwiftUI
import Speech
import Accelerate

class CallViewModel: NSObject, ObservableObject {
    
    @Published var text = ""
    @Published var isRecording = false
    
    private let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private let engine = AVAudioEngine()
    private var request: SFSpeechAudioBufferRecognitionRequest?
    private var task: SFSpeechRecognitionTask?
    private var silenceTimer: Timer?
    
    private let silenceThreshold: Float = 0.01
    private let silenceTime: TimeInterval = 2.0
    
    
    private var audioPlayer: AVAudioPlayer?
    
    // Repository
    private let voicevoxRepository: TextToSpeechRepository
    private let textGenerationRepository: TextGenerationRepository
    
    init(voicevoxRepository: TextToSpeechRepository = VoicevoxRepository(), textGenerationRepository: TextGenerationRepository = OpenAITextGenerationRepository(apiKey: tempAPIKey)) {
        self.voicevoxRepository = voicevoxRepository
        self.textGenerationRepository = textGenerationRepository
    }
    
    // OpenAI LLMçµ±åˆç”¨ã®åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
    convenience init(openAIAPIKey: String) {
        let textGenRepo = OpenAITextGenerationRepository(apiKey: openAIAPIKey)
        self.init(voicevoxRepository: VoicevoxRepository(), textGenerationRepository: textGenRepo)
    }
    
    
    //
    //    var speechRecognizer:SFSpeechRecognizer?
    ////    var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    //    var recognitionTask: SFSpeechRecognitionTask?
    //
    
    func onAppear() {
        Task {
            do {
                // éŸ³å£°ã®èª­ã¿è¾¼ã¿
                guard let asset = NSDataAsset(name: "maou_se_sound_phone02") else {
                    // TODO: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                    print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return
                }
                
                // ç€ä¿¡éŸ³å†ç”Ÿ
                audioPlayer = try AVAudioPlayer(data: asset.data)
                audioPlayer?.numberOfLoops = -1 
                audioPlayer?.prepareToPlay()
                audioPlayer?.play()
                
                // Voicevoxã®åˆæœŸåŒ–
                try await voicevoxRepository.installVoicevox()
                print("Voicevoxã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                try voicevoxRepository.setupSynthesizer()
                
                // main
                try await main()
            } catch {
                print("Voicevoxã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: \(error)")
            }
        }
    }
    
    private func main() async throws {
        let script = try await generateScript()
        let voice = try await generateVoice(script: script)
        Task { @MainActor in
            self.text = script
        }
        try playVoice(data: voice)
    }
    
    
    // ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
    func generateScript() async throws -> String {
        print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ")
        let script = try await textGenerationRepository.generateResponse(userMessage: "")
        print(script)
        return script
    }

    // éŸ³å£°åˆæˆ
    func generateVoice(script: String) async throws -> Data {
        print("éŸ³å£°åˆæˆ")
        let data = try await voicevoxRepository.synthesize(text: script)
        return data
    }
    
    // éŸ³å£°å†ç”Ÿ
    func playVoice(data: Data) throws {
        print("éŸ³å£°å†ç”Ÿ")
        audioPlayer = try AVAudioPlayer(data: data)
        audioPlayer?.delegate = self
        audioPlayer?.prepareToPlay()
        audioPlayer?.play()
    }
    
    // éŸ³å£°èªè­˜é–‹å§‹
    func startSpeachRecognition() {
        print("ğŸ¤ éŸ³å£°èªè­˜é–‹å§‹")
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .mixWithOthers])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("âœ… éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šå®Œäº†")
        } catch {
            print("âŒ éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚¨ãƒ©ãƒ¼: \(error)")
            return
        }
        
        // éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
        request = SFSpeechAudioBufferRecognitionRequest()
        request?.shouldReportPartialResults = true
        print("âœ… éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆå®Œäº†")
        
        let input = engine.inputNode
        let format = input.outputFormat(forBus: 0)
        
        print("ğŸ“Š éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ - ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: \(format.sampleRate)Hz, ãƒãƒ£ãƒãƒ«æ•°: \(format.channelCount)")
        
        input.removeTap(onBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: format) { buf, _ in
            self.request?.append(buf)
//            self.detectSilence(buf)
        }
        print("âœ… éŸ³å£°ã‚¿ãƒƒãƒ—è¨­å®šå®Œäº†")
        
        // éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        task = recognizer?.recognitionTask(with: request!) { result, error in
            if let error = error {
                print("âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
                return
            }
            
            if let result = result {
                let recognizedText = result.bestTranscription.formattedString
                print("ğŸ—£ï¸ èªè­˜çµæœ: \(recognizedText)")
                print("ğŸ“ èªè­˜çŠ¶æ…‹: \(result.isFinal ? "æœ€çµ‚" : "é€”ä¸­")")
                
                DispatchQueue.main.async { 
                    self.text = recognizedText
                }
                
//                if result.isFinal {
//                    print("âœ… éŸ³å£°èªè­˜å®Œäº†")
//                    self.stop()
//                }
                
                
                print("ğŸ”‡ ç„¡éŸ³æ¤œå‡º - ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ï¼ˆ\(self.silenceTime)ç§’å¾Œã«å‡¦ç†å®Ÿè¡Œï¼‰")
                self.silenceTimer?.invalidate()
                self.silenceTimer = Timer.scheduledTimer(withTimeInterval: self.silenceTime, repeats: false) { _ in
                    print("â° 2ç§’ä»¥ä¸Šã®ç„¡éŸ³ãŒç™ºç”Ÿã—ã¾ã—ãŸ - éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™")
                    self.stop()
                }
            }
        }
        print("âœ… éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯é–‹å§‹")
        
        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®é–‹å§‹
        do {
            try engine.start()
            print("âœ… éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹æˆåŠŸ")
            isRecording = true
        } catch {
            print("âŒ éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: \(error)")
        }
    }
    
//    /// éŸ³å£°ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰ç„¡éŸ³ã‚’æ¤œå‡ºã—ã€ä¸€å®šæ™‚é–“ç„¡éŸ³ãŒç¶šã„ãŸã‚‰å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹
//    /// - Parameter buffer: éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€PCMãƒãƒƒãƒ•ã‚¡
//    /// 
//    /// å‹•ä½œ:
//    /// 1. RMSï¼ˆRoot Mean Squareï¼‰ã‚’è¨ˆç®—ã—ã¦éŸ³å£°ã®æŒ¯å¹…ãƒ¬ãƒ™ãƒ«ã‚’æ¸¬å®š
//    /// 2. silenceThreshold (0.01) æœªæº€ãªã‚‰ç„¡éŸ³ã¨åˆ¤å®š
//    /// 3. ç„¡éŸ³ãŒ silenceTime (2.0ç§’) ç¶šã„ãŸã‚‰å‡¦ç†å®Ÿè¡Œ
//    /// 4. éŸ³å£°ãŒæ¤œå‡ºã•ã‚ŒãŸã‚‰ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
//    private func detectSilence(_ buffer: AVAudioPCMBuffer) {
//        return
//        guard let data = buffer.floatChannelData?[0] else {
//            print("âš ï¸ éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—")
//            return 
//        }
//        
//        // RMS (Root Mean Square) è¨ˆç®—: éŸ³å£°ã®æŒ¯å¹…ãƒ¬ãƒ™ãƒ«ã‚’0.0ã€œ1.0ã§è¡¨ç¾
//        let rms = sqrt(stride(from: 0, to: Int(buffer.frameLength), by: buffer.stride)
//            .map { data[$0] * data[$0] }.reduce(0,+) / Float(buffer.frameLength))
//        
//        // éŸ³å£°ãƒ¬ãƒ™ãƒ«ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã€å¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆï¼‰
//        // print("ğŸ”Š éŸ³å£°ãƒ¬ãƒ™ãƒ«: \(String(format: "%.4f", rms)) (é–¾å€¤: \(silenceThreshold))")
//        
//        if rms < silenceThreshold {
//            // ç„¡éŸ³æ¤œå‡º: silenceTimeç§’å¾Œã«å‡¦ç†å®Ÿè¡Œ
//            print("ğŸ”‡ ç„¡éŸ³æ¤œå‡º - ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ï¼ˆ\(silenceTime)ç§’å¾Œã«å‡¦ç†å®Ÿè¡Œï¼‰")
//            silenceTimer?.invalidate()
//            silenceTimer = Timer.scheduledTimer(withTimeInterval: silenceTime, repeats: false) { _ in
//                print("â° 2ç§’ä»¥ä¸Šã®ç„¡éŸ³ãŒç™ºç”Ÿã—ã¾ã—ãŸ - éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™")
//                self.stop()
//            }
//        } else {
//            // éŸ³å£°æ¤œå‡º: ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
//            if silenceTimer != nil {
//                print("ğŸ¤ éŸ³å£°æ¤œå‡º - ç„¡éŸ³ã‚¿ã‚¤ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ")
//            }
//            silenceTimer?.invalidate()
//        }
//    }
    
    func stop() {
        print("â¹ï¸ éŸ³å£°èªè­˜åœæ­¢")
        engine.stop()
        engine.inputNode.removeTap(onBus: 0)
        request?.endAudio()
        task?.cancel()
        isRecording = false
        silenceTimer?.invalidate()
        print("âœ… éŸ³å£°èªè­˜åœæ­¢å®Œäº†")
    }
    
    
    // éŸ³å£°åˆæˆãƒ¡ã‚½ãƒƒãƒ‰
    private func synthesizeSpeech(text: String) {
        Task {
            do {
                let audioData = try await voicevoxRepository.synthesize(text: text)
                print("éŸ³å£°åˆæˆå®Œäº†: \(audioData.count) bytes")
            } catch {
                print("éŸ³å£°åˆæˆã‚¨ãƒ©ãƒ¼: \(error)")
            }
        }
    }
    
}

// MARK: - AVAudioPlayerDelegate
extension CallViewModel: AVAudioPlayerDelegate {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        if flag {
            print("éŸ³å£°å†ç”ŸãŒçµ‚äº†ã—ã¾ã—ãŸ")
            // ã“ã“ã§æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
            startSpeachRecognition()
        } else {
            print("éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ")
        }
    }
    
    func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
        print("éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: \(error?.localizedDescription ?? "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")")
    }
}

