import SwiftUI
import Speech
import Accelerate

class CallViewModel: NSObject, ObservableObject {
    
    @Published var text = ""

    private var history = ""
    
    private let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private let engine = AVAudioEngine()
    private var request: SFSpeechAudioBufferRecognitionRequest?
    private var task: SFSpeechRecognitionTask?
    private var silenceTimer: Timer?
    
    private let silenceThreshold: Float = 0.01
    private let silenceTime: TimeInterval = 2.0
    
    
    private var audioPlayer: AVAudioPlayer?
    
    // Repository
    private let voicevoxRepository: TextToSpeechRepository
    private let textGenerationRepository: TextGenerationRepository
    
    init(voicevoxRepository: TextToSpeechRepository = VoicevoxRepository(), textGenerationRepository: TextGenerationRepository = OpenAITextGenerationRepository(apiKey: tempAPIKey)) {
        self.voicevoxRepository = voicevoxRepository
        self.textGenerationRepository = textGenerationRepository
    }
    
    // OpenAI LLMçµ±åˆç”¨ã®åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰
    convenience init(openAIAPIKey: String) {
        let textGenRepo = OpenAITextGenerationRepository(apiKey: openAIAPIKey)
        self.init(voicevoxRepository: VoicevoxRepository(), textGenerationRepository: textGenRepo)
    }
    

    func onAppear() {
        Task {
            do {
                // éŸ³å£°ã®èª­ã¿è¾¼ã¿
                guard let asset = NSDataAsset(name: "maou_se_sound_phone02") else {
                    // TODO: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                    print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    return
                }
                
                // ç€ä¿¡éŸ³å†ç”Ÿ
                audioPlayer = try AVAudioPlayer(data: asset.data)
                audioPlayer?.numberOfLoops = -1 
                audioPlayer?.prepareToPlay()
                audioPlayer?.play()
                
                // Voicevoxã®åˆæœŸåŒ–
                try await voicevoxRepository.installVoicevox()
                print("Voicevoxã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
                try voicevoxRepository.setupSynthesizer()
                
                // main
                try await main()
            } catch {
                print("Voicevoxã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: \(error)")
            }
        }
    }
    
    private func main() async throws {
        let script = try await generateScript()
        let voice = try await generateVoice(script: script)
        Task { @MainActor in
            self.text = script
            self.history += "ãšã‚“ã ã‚‚ã‚“ã€Œ\(script)ã€\n"
        }
        try playVoice(data: voice)
    }
    
    
    // ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ
    func generateScript() async throws -> String {
        print("ã‚¹ã‚¯ãƒªãƒ—ãƒˆç”Ÿæˆ")
        let script = try await textGenerationRepository.generateResponse(userMessage: history)
        print(script)
        return script
    }

    // éŸ³å£°åˆæˆ
    func generateVoice(script: String) async throws -> Data {
        print("éŸ³å£°åˆæˆ")
        let data = try await voicevoxRepository.synthesize(text: script)
        return data
    }
    
    // éŸ³å£°å†ç”Ÿ
    func playVoice(data: Data) throws {
        print("éŸ³å£°å†ç”Ÿ")
        audioPlayer = try AVAudioPlayer(data: data)
        audioPlayer?.delegate = self
        audioPlayer?.prepareToPlay()
        audioPlayer?.play()
    }
    
    // éŸ³å£°èªè­˜é–‹å§‹
    func startSpeachRecognition() {
        print("ğŸ¤ éŸ³å£°èªè­˜é–‹å§‹")
        
        // æ–°ã—ã„éŒ²éŸ³ã®ãŸã‚ã«textã‚’ã‚¯ãƒªã‚¢
        DispatchQueue.main.async {
            self.text = ""
        }
        
        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š
        do {
            let audioSession = AVAudioSession.sharedInstance()
            try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .mixWithOthers])
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            print("âœ… éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šå®Œäº†")
        } catch {
            print("âŒ éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šã‚¨ãƒ©ãƒ¼: \(error)")
            return
        }
        
        // éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
        request = SFSpeechAudioBufferRecognitionRequest()
        request?.shouldReportPartialResults = true
        print("âœ… éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆå®Œäº†")
        
        let input = engine.inputNode
        let format = input.outputFormat(forBus: 0)
        
        print("ğŸ“Š éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ - ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: \(format.sampleRate)Hz, ãƒãƒ£ãƒãƒ«æ•°: \(format.channelCount)")
        
        input.removeTap(onBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: format) { buf, _ in
            self.request?.append(buf)
//            self.detectSilence(buf)
        }
        print("âœ… éŸ³å£°ã‚¿ãƒƒãƒ—è¨­å®šå®Œäº†")
        
        // éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        task = recognizer?.recognitionTask(with: request!) { result, error in
            if let error = error {
                print("âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
                return
            }
            
            if let result = result {
                let recognizedText = result.bestTranscription.formattedString
                print("ğŸ—£ï¸ èªè­˜çµæœ: \(recognizedText)")
                print("ğŸ“ èªè­˜çŠ¶æ…‹: \(result.isFinal ? "æœ€çµ‚" : "é€”ä¸­")")
                
                                if result.isFinal {
                                    print("âœ… éŸ³å£°èªè­˜å®Œäº†")
                                    return
                                }
                
                DispatchQueue.main.async {
                    self.text = recognizedText
                    print("XXX: \(self.text)")
                }
                
                print("ğŸ”‡ ç„¡éŸ³æ¤œå‡º - ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ï¼ˆ\(self.silenceTime)ç§’å¾Œã«å‡¦ç†å®Ÿè¡Œï¼‰")
                self.silenceTimer?.invalidate()
                self.silenceTimer = Timer.scheduledTimer(withTimeInterval: self.silenceTime, repeats: false) { _ in
                    print("â° 2ç§’ä»¥ä¸Šã®ç„¡éŸ³ãŒç™ºç”Ÿã—ã¾ã—ãŸ - éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™")
                    self.stop()
                }
            }
        }
        print("âœ… éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯é–‹å§‹")
        
        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®é–‹å§‹
        do {
            try engine.start()
            print("âœ… éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹æˆåŠŸ")
        } catch {
            print("âŒ éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹ã‚¨ãƒ©ãƒ¼: \(error)")
        }
    }

    func stop() {
        print("â¹ï¸ éŸ³å£°èªè­˜åœæ­¢")
        
        engine.stop()
        engine.inputNode.removeTap(onBus: 0)
        request?.endAudio()
        task?.finish()

        

        print("âœ… éŸ³å£°èªè­˜åœæ­¢å®Œäº†")
        
        self.history += "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã€Œ\(self.text)ã€\n"
        Task {
            do {
                let script = try await generateScript()
                let voice = try await generateVoice(script: script)
                Task { @MainActor in
                    self.text = script
                    self.history += "ãšã‚“ã ã‚‚ã‚“ã€Œ\(script)ã€\n"
                }
                try playVoice(data: voice)
            } catch {
                print("Error: \(error)")
            }
        }

    }
}

// MARK: - AVAudioPlayerDelegate
extension CallViewModel: AVAudioPlayerDelegate {
    func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        if flag {
            print("éŸ³å£°å†ç”ŸãŒçµ‚äº†ã—ã¾ã—ãŸ")
            // ã“ã“ã§æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
            startSpeachRecognition()
        } else {
            print("éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ")
        }
    }
    
    func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
        print("éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: \(error?.localizedDescription ?? "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")")
    }
}

