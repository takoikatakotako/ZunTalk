import SwiftUI
import Speech
import Accelerate

@MainActor
class CallViewModel: NSObject, ObservableObject {

    @Published var text = ""
    @Published var status: CallStatus = .idle
    @Published var conversationDuration: TimeInterval = 0
    @Published var shouldDismiss = false
    
    private var chatMessages: [ChatMessage] = []

    private let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "ja-JP"))
    private let engine = AVAudioEngine()
    private var request: SFSpeechAudioBufferRecognitionRequest?
    private var task: SFSpeechRecognitionTask?
    private var silenceTimer: Timer?
    private var conversationTimer: Timer?
    
    private let silenceTime: TimeInterval = 2


    private var audioPlayer: AVAudioPlayer?
    private var playbackContinuation: CheckedContinuation<Bool, Never>?
    private var recognitionContinuation: CheckedContinuation<String, Never>?
    private var speechRecognitionStartTime: Date?
    private var mainTask: Task<Void, Never>?

    // Repository
    private let voicevoxRepository: TextToSpeechRepository
    private let textGenerationRepository: TextGenerationRepository
    
    private let prompt = """
        ã‚ãªãŸã¯ãšã‚“ã ã®å¦–ç²¾ã®ãšã‚“ã ã‚‚ã‚“ã§ã™ã€‚èªå°¾ã«ã€Œãªã®ã ã€ã‚’ã¤ã‘ã€è¦ªã—ã¿ã‚„ã™ãæ¥½ã—ã„å£èª¿ã§è©±ã—ã¦ãã ã•ã„ã€‚
        ä»Šã¯é›»è©±ãŒã‹ã‹ã£ã¦ãã¦å—ã‘å–ã£ãŸã¨ã“ã‚ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã¾ã™ã€‚
        æœ€åˆã®ã‚»ãƒªãƒ•ã¯å¿…ãšã€Œé›»è©±ã‚’å—ã‘ãŸæ„Ÿã®ã‚ã‚‹æŒ¨æ‹¶ã€ã«ã—ã¦ãã ã•ã„ã€‚
        ä¾‹: ã€Œã‚‚ã—ã‚‚ã—ã€œï¼Ÿãšã‚“ã ã‚‚ã‚“ãªã®ã ï¼ã€ã€ã€Œã¯ã„ã¯ã€œã„ã€ãšã‚“ã ã‚‚ã‚“ãªã®ã ï¼ã€ã€ã€ŒãŠé›»è©±ã‚ã‚ŠãŒã¨ã†ãªã®ã ï¼ã€ãªã©ã€‚
        ä¾‹ã‚’å‚è€ƒã«ã—ã¤ã¤ã€æ¯å›å°‘ã—é•ã†è¨€ã„å›ã—ã«ã—ã¦ãã ã•ã„ã€‚
        æš´åŠ›çš„ãƒ»æ”»æ’ƒçš„ãƒ»ä¸å¿«ãªç™ºè¨€ã¯ã—ãªã„ã§ãã ã•ã„ã€‚
        """
    
    init(voicevoxRepository: TextToSpeechRepository = VoicevoxRepository(), textGenerationRepository: TextGenerationRepository = OpenAITextGenerationRepository(apiKey: tempAPIKey)) {
        self.voicevoxRepository = voicevoxRepository
        self.textGenerationRepository = textGenerationRepository
    }

    func onAppear() {
        guard status == .idle else {
            print("idleä»¥å¤–ã‹ã‚‰å‘¼ã°ã‚Œã¾ã—ãŸ")
            return
        }

        mainTask = Task {
            do {
                try await main()
            } catch {
                print("Voicevoxã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: \(error)")
            }
        }
    }

    func requestDismiss() {
        print("ğŸ“± é€šè©±çµ‚äº†ãƒªã‚¯ã‚¨ã‚¹ãƒˆ")

        // ä¼šè©±ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
        mainTask?.cancel()
        mainTask = nil

        // ã‚¿ã‚¤ãƒãƒ¼ã‚’åœæ­¢
        silenceTimer?.invalidate()
        conversationTimer?.invalidate()

        // éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢
        task?.cancel()
        task?.finish()
        task = nil
        request?.endAudio()
        request = nil

        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åœæ­¢
        if engine.isRunning {
            engine.stop()
            engine.inputNode.removeTap(onBus: 0)
        }

        // éŸ³å£°å†ç”Ÿã‚’åœæ­¢
        audioPlayer?.stop()

        // ä¼šè©±å±¥æ­´ã‚’å‰Šé™¤
        chatMessages.removeAll()

        // VOICEVOXã‚’ã‚¯ãƒªãƒ¼ãƒ³ãƒŠãƒƒãƒ—
        voicevoxRepository.cleanupSynthesizer()

        // dismissã‚’ãƒˆãƒªã‚¬ãƒ¼
        shouldDismiss = true
    }
    
    private func main() async throws {
        // initializingVoiceVox
        status = .initializingVoiceVox
        try await initializingVoiceVox()
        guard !shouldDismiss else { return }

        // requestingPermission
        status = .requestingPermission
        let result = await requestSpeechRecognitionPermission()
        guard result else {
            // è¨±å¯å¾—ã‚‰ã‚Œãªã‹ã£ãŸ
            print("è¨±å¯å¾—ã‚‰ã‚Œãªã‹ã£ãŸã§ã™")
            return
        }
        guard !shouldDismiss else { return }

        // Play Incoming Call
        try playIncomingCall()
        guard !shouldDismiss else { return }

        // Generate Script
        status = .generatingScript
        assert(chatMessages.isEmpty)
        chatMessages.append(ChatMessage(role: .system, content: prompt))
        let script = try await generateScript(inputs: chatMessages)
        guard !shouldDismiss else { return }

        // Generate Voice
        let voice = try await generateVoice(script: script)
        guard !shouldDismiss else { return }

        // Stop Incomint Call
        stopIncomingCall()

        // ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›´
        text = script

        // ä¼šè©±æ™‚é–“æ¸¬å®šé–‹å§‹
        speechRecognitionStartTime = Date()
        startConversationTimer()

        // Play Voice
        try await playVoice(data: voice)
        guard !shouldDismiss else { return }

        //
        try await conversation()
    }

    private func conversation() async throws {
        guard !shouldDismiss else { return }

        // Start Speech Recognition
        let recognizedText = try await startSpeachRecognition()
        print("èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: \(recognizedText)")
        guard !shouldDismiss else { return }

        // ä¼šè©±æ™‚é–“ã‚’ç¢ºèª
        if let startTime = speechRecognitionStartTime {
            let elapsedTime = Date().timeIntervalSince(startTime)
            if elapsedTime >= 60 {
                print("â±ï¸ ä¼šè©±æ™‚é–“ãŒ1åˆ†ä»¥ä¸Šã§ã™: \(Int(elapsedTime))ç§’")
                // çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
                try await endConversation()
                return
            }
        }

        status = .processingResponse
        chatMessages.append(ChatMessage(role: .user, content: recognizedText))

        status = .generatingScript
        let script = try await generateScript(inputs: chatMessages)
        guard !shouldDismiss else { return }

        let voice = try await generateVoice(script: script)
        guard !shouldDismiss else { return }

        chatMessages.append(ChatMessage(role: .assistant, content: script))
        text = script

        try await playVoice(data: voice)
        guard !shouldDismiss else { return }

        // æ¬¡ã®ä¼šè©±ã¸
        try await conversation()
    }

    private func endConversation() async throws {
        print("ğŸ”š ä¼šè©±ã‚’çµ‚äº†ã—ã¾ã™")

        // çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ 
        chatMessages.append(ChatMessage(role: .system, content: "ä¼šè©±æ™‚é–“ãŒ1åˆ†ã‚’è¶…ãˆãŸã®ã§ã€ãšã‚“ã ã‚‚ã‚“ã‚‰ã—ãè¦ªã—ã¿ã‚„ã™ã„æŒ¨æ‹¶ã§ä¼šè©±ã‚’çµ‚äº†ã—ã¦ãã ã•ã„ã€‚"))

        status = .generatingScript
        let script = try await generateScript(inputs: chatMessages)

        let voice = try await generateVoice(script: script)

        chatMessages.append(ChatMessage(role: .assistant, content: script))
        text = script

        try await playVoice(data: voice)

        // ä¼šè©±çµ‚äº†
        status = .ended
        conversationTimer?.invalidate()
        print("âœ… ä¼šè©±ãŒçµ‚äº†ã—ã¾ã—ãŸ")
    }

    private func initializingVoiceVox() async throws {
        // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        guard status == .initializingVoiceVox else {
            fatalError("initializingVoiceVoxä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã™")
        }
        
        // VOICEVOXã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
        try await voicevoxRepository.installVoicevox()
        print("VoiceVoxã®åˆæœŸåŒ–å®Œäº†")
        
        // VOICEVOXã®ã‚·ãƒ³ã‚»ã‚µã‚¤ã‚¶ãƒ¼ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        try voicevoxRepository.setupSynthesizer()
        print("VOICEVOXã®ã‚·ãƒ³ã‚»ã‚µã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å®Œäº†")
    }
    
    private func requestSpeechRecognitionPermission() async -> Bool {
        // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        guard status == .requestingPermission else {
            fatalError("requestingPermissionä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã™")
        }

        let authStatus = SFSpeechRecognizer.authorizationStatus()

        switch authStatus {
        case .authorized:
            return true
        case .denied, .restricted:
            return false
        case .notDetermined:
            break
        @unknown default:
            return false
        }

        // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«è¨±å¯ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        return await withCheckedContinuation { continuation in
            SFSpeechRecognizer.requestAuthorization { status in
                continuation.resume(returning: status == .authorized)
            }
        }
    }
    
    private func playIncomingCall() throws {
        // éŸ³å£°ã®èª­ã¿è¾¼ã¿
        guard let asset = NSDataAsset(name: "maou_se_sound_phone02") else {
            // TODO: ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
            fatalError("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        }

        // ç€ä¿¡éŸ³å†ç”Ÿ
        audioPlayer = try AVAudioPlayer(data: asset.data)
        audioPlayer?.numberOfLoops = -1
        audioPlayer?.prepareToPlay()
        audioPlayer?.play()
    }
    
    private func generateScript(inputs: [ChatMessage]) async throws -> String {
        // ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        guard status == .generatingScript else {
            fatalError("generatingScriptä»¥å¤–ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã™")
        }

        let script = try await textGenerationRepository.generateResponse(inputs: inputs)
        print(script)
        return script
    }

    private func generateVoice(script: String) async throws -> Data {
        print("éŸ³å£°åˆæˆ")
        status = .synthesizingVoice

        let data = try await voicevoxRepository.synthesize(text: script)
        return data
    }
    
    private func stopIncomingCall() {
        audioPlayer?.stop()
    }
    
    private func playVoice(data: Data) async throws {
        status = .playingVoice

        audioPlayer = try AVAudioPlayer(data: data)
        audioPlayer?.delegate = self
        audioPlayer?.prepareToPlay()
        audioPlayer?.play()

        // å†ç”Ÿçµ‚äº†ã‚’å¾…ã¤
        let success = await withCheckedContinuation { continuation in
            playbackContinuation = continuation
        }

        if !success {
            print("éŸ³å£°å†ç”Ÿã«å¤±æ•—ã—ã¾ã—ãŸ")
        }
    }
    
    // éŸ³å£°èªè­˜é–‹å§‹
    private func startSpeachRecognition() async throws -> String {
        print("ğŸ¤ éŸ³å£°èªè­˜é–‹å§‹")

        status = .recognizingSpeech
        text = ""

        // éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®è¨­å®š
        let audioSession = AVAudioSession.sharedInstance()
        try audioSession.setCategory(.playAndRecord, mode: .voiceChat, options: [.defaultToSpeaker, .mixWithOthers])
        try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
        print("âœ… éŸ³å£°ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®šå®Œäº†")

        // éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ä½œæˆ
        request = SFSpeechAudioBufferRecognitionRequest()
        request?.shouldReportPartialResults = true
        print("âœ… éŸ³å£°èªè­˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆå®Œäº†")

        let input = engine.inputNode
        let format = input.outputFormat(forBus: 0)

        print("ğŸ“Š éŸ³å£°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ - ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: \(format.sampleRate)Hz, ãƒãƒ£ãƒãƒ«æ•°: \(format.channelCount)")

        input.removeTap(onBus: 0)
        input.installTap(onBus: 0, bufferSize: 1024, format: format) { buf, _ in
            self.request?.append(buf)
        }
        print("âœ… éŸ³å£°ã‚¿ãƒƒãƒ—è¨­å®šå®Œäº†")

        // éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯ã®é–‹å§‹
        task = recognizer?.recognitionTask(with: request!) { result, error in
            if let error = error {
                print("âŒ éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: \(error.localizedDescription)")
                return
            }

            guard let result = result else { return }

            let recognizedText = result.bestTranscription.formattedString
            print("ğŸ—£ï¸ èªè­˜çµæœ: \(recognizedText)")
            print("ğŸ“ èªè­˜çŠ¶æ…‹: \(result.isFinal ? "æœ€çµ‚" : "é€”ä¸­")")

            if result.isFinal {
                print("âœ… éŸ³å£°èªè­˜å®Œäº†")
                return
            }

            DispatchQueue.main.async {
                self.text = recognizedText
                print("XXX: \(self.text)")
            }

            print("ğŸ”‡ ç„¡éŸ³æ¤œå‡º - ã‚¿ã‚¤ãƒãƒ¼é–‹å§‹ï¼ˆ\(self.silenceTime)ç§’å¾Œã«å‡¦ç†å®Ÿè¡Œï¼‰")
            self.silenceTimer?.invalidate()
            self.silenceTimer = Timer.scheduledTimer(withTimeInterval: self.silenceTime, repeats: false) { _ in
                print("â° 2ç§’ä»¥ä¸Šã®ç„¡éŸ³ãŒç™ºç”Ÿã—ã¾ã—ãŸ - éŸ³å£°èªè­˜ã‚’åœæ­¢ã—ã¾ã™")
                Task { @MainActor in
                    self.stopRecognition()
                }
            }
        }
        print("âœ… éŸ³å£°èªè­˜ã‚¿ã‚¹ã‚¯é–‹å§‹")

        // éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³ã®é–‹å§‹
        try engine.start()
        print("âœ… éŸ³å£°ã‚¨ãƒ³ã‚¸ãƒ³é–‹å§‹æˆåŠŸ")

        // éŸ³å£°èªè­˜ã®çµ‚äº†ã‚’å¾…ã¤
        return await withCheckedContinuation { continuation in
            recognitionContinuation = continuation
        }
    }

    private func stopRecognition() {
        engine.stop()
        engine.inputNode.removeTap(onBus: 0)
        request?.endAudio()
        task?.finish()

        print("âœ… éŸ³å£°èªè­˜åœæ­¢å®Œäº†")

        // èªè­˜çµæœã‚’è¿”ã™
        recognitionContinuation?.resume(returning: text)
        recognitionContinuation = nil
    }

    private func startConversationTimer() {
        conversationTimer?.invalidate()
        conversationTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { [weak self] _ in
            Task { @MainActor [weak self] in
                guard let self = self, let startTime = self.speechRecognitionStartTime else { return }
                self.conversationDuration = Date().timeIntervalSince(startTime)
            }
        }
    }
}

// MARK: - AVAudioPlayerDelegate
extension CallViewModel: AVAudioPlayerDelegate {
    nonisolated func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
        Task { @MainActor in
            playbackContinuation?.resume(returning: flag)
            playbackContinuation = nil
        }
    }

    nonisolated func audioPlayerDecodeErrorDidOccur(_ player: AVAudioPlayer, error: Error?) {
        Task { @MainActor in
            print("éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: \(error?.localizedDescription ?? "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")")
            playbackContinuation?.resume(returning: false)
            playbackContinuation = nil
        }
    }
}

